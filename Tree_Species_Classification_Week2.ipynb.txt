{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v6L1J_lRlQ2",
        "outputId": "920ef099-6e32-4e6c-cf7c-50129a3cda3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n",
            "‚è≥ Downloading dataset...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/viditgandhi/tree-species-identification-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72.8M/72.8M [00:00<00:00, 193MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Path to dataset files: /root/.cache/kagglehub/datasets/viditgandhi/tree-species-identification-dataset/versions/1\n",
            "‚úÖ Dataset copied successfully to: /content/tree-species-dataset\n",
            "\n",
            "üìÇ Files in the dataset folder:\n",
            "['Tree_Species_Dataset']\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub\n",
        "import kagglehub, shutil, os\n",
        "\n",
        "print(\"‚è≥ Downloading dataset...\")\n",
        "dataset_path = kagglehub.dataset_download(\"viditgandhi/tree-species-identification-dataset\")\n",
        "print(\"‚úÖ Path to dataset files:\", dataset_path)\n",
        "\n",
        "# ‚úÖ Copy the dataset to a simple path\n",
        "destination_path = '/content/tree-species-dataset'\n",
        "shutil.copytree(dataset_path, destination_path, dirs_exist_ok=True)\n",
        "print(\"‚úÖ Dataset copied successfully to:\", destination_path)\n",
        "\n",
        "print(\"\\nüìÇ Files in the dataset folder:\")\n",
        "print(os.listdir(destination_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = \"/content/tree-species-dataset/Tree_Species_Dataset/train\"\n",
        "val_dir = \"/content/tree-species-dataset/Tree_Species_Dataset/val\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "        target_size=(224, 224),\n",
        "            batch_size=32,\n",
        "                class_mode='categorical'\n",
        "\n",
        "        )\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,                target_size=(224, 224),\n",
        "                            batch_size=32,\n",
        "                                class_mode='categorical'\n",
        "                                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "OAPYqnbRSZHt",
        "outputId": "7305f88a-11cb-4c7e-9cce-9d50b01e81ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-2-1952507196.py, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2-1952507196.py\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    val_generator = val_datagen.flow_from_directory(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = \"/content/tree-species-dataset/Tree_Species_Dataset/train\"\n",
        "val_dir = \"/content/tree-species-dataset/Tree_Species_Dataset/val\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "        target_size=(224, 224),\n",
        "            batch_size=32,\n",
        "                class_mode='categorical'\n",
        "                )\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        "                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yAnOC7lUTT-a",
        "outputId": "bb2346dc-db27-4177-bcf9-d698174f4508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/tree-species-dataset/Tree_Species_Dataset/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3079514009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mval_datagen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tree-species-dataset/Tree_Species_Dataset/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"üìÇ Main dataset folder:\", os.listdir(\"/content/tree-species-dataset/Tree_Species_Dataset\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msn-eaFLT6AN",
        "outputId": "1bb9df5f-3e67-4a70-9970-98b2f377a884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Main dataset folder: ['champa', 'bili', 'pipal', 'kesudo', 'shirish', 'bamboo', 'other', 'motichanoti', 'khajur', 'gunda', 'cactus', 'mango', 'gulmohor', 'jamun', 'banyan', 'saptaparni', 'neem', 'sonmahor', 'babul', 'pilikaren', 'asopalav', 'vad', 'nilgiri', 'kanchan', 'sitafal', 'sugarcane', 'simlo', 'garmalo', 'amla', 'coconut', '.git']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "base_dir = \"/content/tree-species-dataset/Tree_Species_Dataset\"\n",
        "train_dir = \"/content/tree-species-dataset/train\"\n",
        "val_dir = \"/content/tree-species-dataset/val\"\n",
        "\n",
        "# Create train and val directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Split each species folder into train and val\n",
        "split_ratio = 0.8  # 80% train, 20% validation\n",
        "\n",
        "for species in os.listdir(base_dir):\n",
        "    species_path = os.path.join(base_dir, species)\n",
        "        if os.path.isdir(species_path):\n",
        "                images = os.listdir(species_path)\n",
        "                        random.shuffle(images)\n",
        "\n",
        "                                split_index = int(len(images) * split_ratio)\n",
        "                                        train_images = images[:split_index]\n",
        "                                                val_images = images[split_index:]\n",
        "\n",
        "                                                        # Create species folders in train and val directories\n",
        "                                                                os.makedirs(os.path.join(train_dir, species), exist_ok=True)\n",
        "                                                                        os.makedirs(os.path.join(val_dir, species), exist_ok=True)\n",
        "\n",
        "                                                                                # Move images to respective folders\n",
        "                                                                                        for img in train_images:\n",
        "                                                                                                    shutil.copy(os.path.join(species_path, img), os.path.join(train_dir, species, img))\n",
        "\n",
        "                                                                                                            for img in val_images:\n",
        "                                                                                                                        shutil.copy(os.path.join(species_path, img), os.path.join(val_dir, species, img))\n",
        "\n",
        "                                                                                                                        print(\"‚úÖ Dataset successfully split into Train and Validation folders!\")\n",
        "                                                                                                                        print(\"Train folders:\", os.listdir(train_dir))\n",
        "                                                                                                                        print(\"Val folders:\", os.listdir(val_dir))"
      ],
      "metadata": {
        "id": "2o8b4R68Vdte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "base_dir = \"/content/tree-species-dataset/Tree_Species_Dataset\"\n",
        "train_dir = \"/content/tree-species-dataset/train\"\n",
        "val_dir = \"/content/tree-species-dataset/val\"\n",
        "\n",
        "# Create train and val directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Split each species folder into train and val\n",
        "split_ratio = 0.8  # 80% train, 20% validation\n",
        "\n",
        "for species in os.listdir(base_dir):\n",
        "    species_path = os.path.join(base_dir, species)\n",
        "    if os.path.isdir(species_path):\n",
        "        images = os.listdir(species_path)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        split_index = int(len(images) * split_ratio)\n",
        "        train_images = images[:split_index]\n",
        "        val_images = images[split_index:]\n",
        "\n",
        "        # Create species folders in train and val directories\n",
        "        os.makedirs(os.path.join(train_dir, species), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, species), exist_ok=True)\n",
        "\n",
        "        # Move images to respective folders\n",
        "        for img in train_images:\n",
        "            shutil.copy(os.path.join(species_path, img), os.path.join(train_dir, species, img))\n",
        "\n",
        "        for img in val_images:\n",
        "            shutil.copy(os.path.join(species_path, img), os.path.join(val_dir, species, img))\n",
        "\n",
        "print(\"‚úÖ Dataset successfully split into Train and Validation folders!\")\n",
        "print(\"Train folders:\", os.listdir(train_dir))\n",
        "print(\"Val folders:\", os.listdir(val_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hWmT7U6lVqOI",
        "outputId": "bb40ff11-099b-4579-ddc8-6daeb2468885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "[Errno 21] Is a directory: '/content/tree-species-dataset/Tree_Species_Dataset/.git/hooks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3842519135.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Move images to respective folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content/tree-species-dataset/Tree_Species_Dataset/.git/hooks'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "base_dir = \"/content/tree-species-dataset/Tree_Species_Dataset\"\n",
        "train_dir = \"/content/tree-species-dataset/train\"\n",
        "val_dir = \"/content/tree-species-dataset/val\"\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "split_ratio = 0.8  # 80% train, 20% val\n",
        "\n",
        "for species in os.listdir(base_dir):\n",
        "    species_path = os.path.join(base_dir, species)\n",
        "    if os.path.isdir(species_path):\n",
        "        # ‚úÖ Only take actual image files (ignore folders)\n",
        "        images = [img for img in os.listdir(species_path)\n",
        "                          if os.path.isfile(os.path.join(species_path, img))]\n",
        "        random.shuffle(images)\n",
        "\n",
        "        split_index = int(len(images) * split_ratio)\n",
        "        train_images = images[:split_index]\n",
        "        val_images = images[split_index:]\n",
        "\n",
        "        os.makedirs(os.path.join(train_dir, species), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, species), exist_ok=True)\n",
        "\n",
        "        # ‚úÖ Copy only image files\n",
        "        for img in train_images:\n",
        "            shutil.copy(os.path.join(species_path, img), os.path.join(train_dir, species, img))\n",
        "\n",
        "        for img in val_images:\n",
        "            shutil.copy(os.path.join(species_path, img), os.path.join(val_dir, species, img))\n",
        "\n",
        "print(\"‚úÖ Dataset successfully split into Train and Validation folders!\")\n",
        "print(\"Train folders:\", os.listdir(train_dir))\n",
        "print(\"Val folders:\", os.listdir(val_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaWgevdPWCAP",
        "outputId": "f5b15c21-7711-4b53-d42a-252584d87bf2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset successfully split into Train and Validation folders!\n",
            "Train folders: ['champa', 'bili', 'pipal', 'kesudo', 'shirish', 'bamboo', 'other', 'motichanoti', 'khajur', 'gunda', 'cactus', 'mango', 'gulmohor', 'jamun', 'banyan', 'saptaparni', 'neem', 'sonmahor', 'babul', 'pilikaren', 'asopalav', 'vad', 'nilgiri', 'kanchan', 'sitafal', 'sugarcane', 'simlo', 'garmalo', 'amla', 'coconut', '.git']\n",
            "Val folders: ['champa', 'bili', 'pipal', 'kesudo', 'shirish', 'bamboo', 'other', 'motichanoti', 'khajur', 'gunda', 'cactus', 'mango', 'gulmohor', 'jamun', 'banyan', 'saptaparni', 'neem', 'sonmahor', 'babul', 'pilikaren', 'asopalav', 'vad', 'nilgiri', 'kanchan', 'sitafal', 'sugarcane', 'simlo', 'garmalo', 'amla', 'coconut', '.git']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# ‚úÖ Paths\n",
        "train_dir = \"/content/tree-species-dataset/train\"\n",
        "val_dir = \"/content/tree-species-dataset/val\"\n",
        "\n",
        "# ‚úÖ Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ‚úÖ Build Model (Transfer Learning)\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze base model\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.3),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ‚úÖ Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[\n",
        "        EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(factor=0.5, patience=3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ‚úÖ Save the Trained Model\n",
        "model.save(\"efficient_tree_model.h5\")\n",
        "print(\"‚úÖ Model saved as efficient_tree_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8qDzJ3XHsV",
        "outputId": "e5ae338c-4e5b-4d6d-843e-04bffaf1867f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1523 images belonging to 31 classes.\n",
            "Found 563 images belonging to 31 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.0891 - loss: 3.7196 - val_accuracy: 0.3908 - val_loss: 2.2306 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.3464 - loss: 2.3106 - val_accuracy: 0.5684 - val_loss: 1.6214 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.4652 - loss: 1.8258 - val_accuracy: 0.6554 - val_loss: 1.3249 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.5374 - loss: 1.5896 - val_accuracy: 0.6714 - val_loss: 1.1914 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.5707 - loss: 1.4489 - val_accuracy: 0.7229 - val_loss: 1.0492 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.6524 - loss: 1.2039 - val_accuracy: 0.7194 - val_loss: 0.9959 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.6874 - loss: 1.1733 - val_accuracy: 0.7513 - val_loss: 0.8970 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 3s/step - accuracy: 0.6917 - loss: 1.0826 - val_accuracy: 0.7815 - val_loss: 0.8496 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.7156 - loss: 1.0320 - val_accuracy: 0.7904 - val_loss: 0.7942 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 2s/step - accuracy: 0.7221 - loss: 0.9788 - val_accuracy: 0.8082 - val_loss: 0.7389 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - accuracy: 0.7191 - loss: 0.9794 - val_accuracy: 0.7975 - val_loss: 0.7362 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.7536 - loss: 0.8804 - val_accuracy: 0.7957 - val_loss: 0.7074 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - accuracy: 0.7506 - loss: 0.8567 - val_accuracy: 0.8153 - val_loss: 0.6728 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.7322 - loss: 0.8810 - val_accuracy: 0.8224 - val_loss: 0.6600 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.7682 - loss: 0.8181 - val_accuracy: 0.8259 - val_loss: 0.6459 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.7788 - loss: 0.8072 - val_accuracy: 0.8135 - val_loss: 0.6469 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 2s/step - accuracy: 0.7640 - loss: 0.7600 - val_accuracy: 0.8135 - val_loss: 0.6488 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - accuracy: 0.7754 - loss: 0.7557 - val_accuracy: 0.8206 - val_loss: 0.6492 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.7909 - loss: 0.6795 - val_accuracy: 0.8384 - val_loss: 0.5694 - learning_rate: 5.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.8084 - loss: 0.6868 - val_accuracy: 0.8401 - val_loss: 0.5627 - learning_rate: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model saved as efficient_tree_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1) Zip only the dataset (train + val)\n",
        "shutil.make_archive(\"tree_species_dataset\", 'zip', \"/content/tree-species-dataset\")\n",
        "\n",
        "# 2) Create a folder for final submission\n",
        "os.makedirs(\"final_submission\", exist_ok=True)\n",
        "\n",
        "# Move required files into the folder\n",
        "os.rename(\"/content/tree_species_dataset.zip\", \"final_submission/tree_species_dataset.zip\")\n",
        "os.rename(\"/efficient_tree_model.h5\", \"final_submission/efficient_tree_model.h5\")\n",
        "\n",
        "# Copy notebook (replace with your notebook name)\n",
        "import shutil\n",
        "# Get the current notebook filename\n",
        "notebook_filename = !echo \"$@\"\n",
        "notebook_filename = notebook_filename[0].split('/')[-1]\n",
        "shutil.copy(notebook_filename, \"final_submission/\" + notebook_filename)\n",
        "\n",
        "# 3) Zip only the final submission folder\n",
        "shutil.make_archive(\"Tree_Species_Week2_Submission\", 'zip', \"final_submission\")\n",
        "\n",
        "print(\"‚úÖ Final zip created: Tree_Species_Week2_Submission.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "qlayfncSi1HT",
        "outputId": "a3156502-5328-4bf7-9bb4-4386ea4325f4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'efficient_tree_model.h5' -> 'final_submission/efficient_tree_model.h5'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-1015850695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Move required files into the folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/tree_species_dataset.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_submission/tree_species_dataset.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"efficient_tree_model.h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_submission/efficient_tree_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Copy notebook (replace with your notebook name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'efficient_tree_model.h5' -> 'final_submission/efficient_tree_model.h5'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Current directory files:\", os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r2h3K-BmELo",
        "outputId": "f2ff0cc3-deb3-40aa-896a-ec7553ed911a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current directory files: ['.config', 'final_submission', 'Tree_Species_Week2_Submission.zip', 'tree-species-dataset', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Files:\", os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egfNUilenU3q",
        "outputId": "df26fd7b-2825-45d1-d5df-6b11f6b5579a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files: ['.config', 'final_submission', 'Tree_Species_Week2_Submission.zip', 'tree-species-dataset', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"efficient_tree_model.h5\")\n",
        "print(\"‚úÖ Model saved as efficient_tree_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrd1CnVKoIdd",
        "outputId": "c1e3a3ed-9ea2-4ae8-bd03-6a6bcaf85c2b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model saved as efficient_tree_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"tree_species_dataset\", 'zip', \"tree-species-dataset\")\n",
        "print(\"‚úÖ Dataset zipped as tree_species_dataset.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE69ghacoq18",
        "outputId": "5a7c5517-ade5-44ea-c074-a81e588436ba"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset zipped as tree_species_dataset.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Create a folder for final submission\n",
        "!mkdir final_submission\n",
        "\n",
        "# Move all required files into it\n",
        "!cp efficient_tree_model.h5 final_submission/\n",
        "!cp tree_species_dataset.zip final_submission/\n",
        "!cp *.ipynb final_submission/\n",
        "\n",
        "# Create the final submission zip\n",
        "shutil.make_archive(\"Tree_Species_Week2_Submission\", 'zip', 'final_submission')\n",
        "print(\"‚úÖ Final zip created: Tree_Species_Week2_Submission.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TRf5iIYpE29",
        "outputId": "ea5f72d0-a369-4c5e-bf5f-49a465611214"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòfinal_submission‚Äô: File exists\n",
            "cp: cannot stat '*.ipynb': No such file or directory\n",
            "‚úÖ Final zip created: Tree_Species_Week2_Submission.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "[ipynb for ipynb in os.listdir() if ipynb.endswith('.ipynb')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxDL5sPgppPJ",
        "outputId": "b7a85200-3919-4a1a-8492-d04ca9bd0171"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}